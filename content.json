{"posts":[{"title":"follow-verify","text":"This message is used to verify that this feed (feedId:69247643610012672) belongs to me (userId:62034052875345920). Join me in enjoying the next generation information browser https://follow.is.","link":"/post/follow-verify/"},{"title":"抽象类、普通类、接口的一些思考","text":"2023年1月11日 最近重构代码时，希望一些通用的功能被多个子类复用。第一时间想到的是抽出为工具类，但工具类是静态类，而此功能需要动态注入一些资源文件，导致此种方法作罢。 其次就是继承，将通用的方法写入父类中，子类通过继承来使用通用的方法。但CR时发现此父类应该是抽象类，而不是普通类。进而引发了对抽象类、普通类、接口的一些思考。 抽象类的应用场景第一种场景通用的功能被多个子类复用的时候 比如说，AbstractPlayer 抽象类中有一个普通的方法 sleep()，表明所有运动员都需要休息，那么这个方法就可以被子类复用。 12345abstract class AbstractPlayer { public void sleep() { System.out.println(&quot;运动员也要休息而不是挑战极限&quot;); }} 子类 BasketballPlayer 继承了 AbstractPlayer 类： 12class BasketballPlayer extends AbstractPlayer {} 也就拥有了 sleep() 方法。BasketballPlayer 的对象可以直接调用父类的 sleep() 方法： 12BasketballPlayer basketballPlayer = new BasketballPlayer();basketballPlayer.sleep(); 子类 FootballPlayer 继承了 AbstractPlayer 类： 12class FootballPlayer extends AbstractPlayer {} 也拥有了 sleep() 方法，FootballPlayer 的对象也可以直接调用父类的 sleep() 方法： 12FootballPlayer footballPlayer = new FootballPlayer();footballPlayer.sleep(); 这样是不是就实现了代码的复用。 第二种场景在抽象类中定义好接口，然后子类中去实现。 比如说，AbstractPlayer 抽象类中定义了一个抽象方法 play()，表明所有运动员都可以从事某项运动，但需要对应子类去扩展实现，表明篮球运动员打篮球，足球运动员踢足球。 123abstract class AbstractPlayer { abstract void play();} BasketballPlayer 继承了 AbstractPlayer 类，扩展实现了自己的 play() 方法。 123456public class BasketballPlayer extends AbstractPlayer { @Override void play() { System.out.println(&quot;我是张伯伦，我篮球场上得过 100 分，&quot;); }} FootballPlayer 继承了 AbstractPlayer 类，扩展实现了自己的 play() 方法。 123456public class FootballPlayer extends AbstractPlayer { @Override void play() { System.out.println(&quot;我是C罗，我能接住任意高度的头球&quot;); }} 抽象类与普通类 问： 把父类改成抽象类，方法改成抽象方法，那么public void play(); 子类不变，依然重写父类方法，那这个跟普通父类没区别啊？难道说就一个抽象方法没方法体就完事了？？那我普通方法有方法体，我空着不写内容不就得了，不跟抽象方法一个样吗？？别跟我说抽象类还不能实例化，哥也不需要去new它！普通类都能搞定的，还弄个抽象类有什么意义？我前面都说了普通类的方法我可以空着不写，达到跟抽象类方法没方法体一样的效果。既然两种方式都能达到同一个输出效果，弄一种方式不就得了，那为什么还要创造出一个抽象类出来？难道是比普通类看着舒服？用着爽？还是更加便捷？还是为了强制让别人用的时候必须强制化实现抽象方法省的你忘了什么的？ 答： 就是为了强制不能实例化，以及强制子类必须实现方法这不是你忘不忘的问题，你说你不去new它就行了，这话没错。那你想另一个问题，为什么要有访问控制呢？为什么要有private和public之分呢？我可以全部public，不该访问的，我不访问就行了啊？小程序里，看不出什么区别，反而private成员要写一堆set和get函数，多麻烦，我自己写小程序的时候也会偷懒全部public，但是项目大了，代码多了，这种严谨的结构就很重要了。且不说会有很多人合作一起写一个程序，哪怕还是你一个人写，也保不住有忘记的时候，那时候编译器不报错，茫茫码海上哪找错误去面向对象说到底就是方便你思考，易扩展、易维护管理，硬要说没必要，整个面向对象都没必要了，C语言有什么干不了的呀，运行效率还高。 总而言之使用继承的时候，一定要继承抽象类，或者抽象类的子类。 抽象类与接口接口是对动作的抽象，抽象类是对本质的抽象。 抽象类表示的是，这个对象是什么。接口表示的是，这个对象能做什么。比如，男人和女人，他们的抽象类是人类，而猫和狗的抽象类是宠物类。人类可以吃东西，宠物类也可以吃东西，但是两者不能混为一谈，因为有本质的区别。这个“吃东西”是一个动作，你可以把“吃东西”定义成一个接口，然后让两个类去实现它的方法。 所以，在高级语言上，一个类只能继承一个类或抽象类，正如人不可能同时是动物类又是植物类，但是可以实现多个接口，例如，吃饭接口、呼吸接口等。 使用差异a.抽象类 和 接口 都是用来抽象具体对象的，但是接口的抽象级别最高； b.抽象类可以有具体的方法和属性, 接口只能有抽象方法和==不可变常量(final)==； c.抽象类主要用来抽象类别,接口主要用来抽象功能； d.抽象类中不包含任何实现，派生类必须覆盖它们。接口中所有方法都必须是未实现的； e.抽象类实现接口时，接口的方法在抽象类中可以被实现也可以不被实现，而普通实现接口必须实现所有接口方法。 使用方向 当你关注一个事物的本质的时候，用抽象类； 当你关注一个操作的时候，用接口。 小结抽象类的功能要远超过接口，但是，定义抽象类的代价高。因为高级语言来说（从实际设计上来说也是）每个类只能继承一个类。在这个类中，你必须继承或编写出其所有子类的所有共性。虽然接口在功能上会弱化许多，但是它只是针对一个动作的描述。而且你可以在一个类中同时实现多个接口。在设计阶段会降低难度的。 参考Java 抽象类、普通类、接口的区别——值得你一看的干货Java抽象类，看这一篇就够了，豁然开朗","link":"/post/reflections-on-abstract-classes-regular-classes-and-interfaces/"},{"title":"提高elasticSearch搜索的准确率","text":"全文检索在信息检索领域具有重要意义，然而，在实际工作中，直接使用es+ik分词器的组合，会使得搜索结果的准确度会大打折扣。本文针对这一问题进行了深入分析，并提出了多项优化策略，旨在提高搜索结果的精准度和用户体验。 当搜索词不能被正确分词时，搜索结果劣化上传一个文档包含测试表格内容搜索时，再进行搜索 可以看到目标十分靠后。 原因分析产生这个现象的原因和elasticSearch的分词有关。众所周知es有个分词器，将保存的文本进行分词后建立索引。 以上述 测试表格内容搜索 这个关键词为例，假定会被分为“测试”、“表格”、“内容”、“搜索”这四个词。在用测试表格内容搜索 作为关键词搜索时，es就会使用上述的四个词，作为查询条件去倒排索引中匹配文档。找到包含至少一个关键词的文档。并且计算每个文档的相关度得分，并根据得分进行排序。最后，返回匹配的文档以及相关度得分的排名结果。 所以出现了搜索测试表格内容搜索 出现了表格、测试等这些零散的关键词，而用户搜索想要的结果却排在后面。 优化es评分策略针对当前问题如搜索硬件设备需求表时，结果很多是关于“硬件”、“设备”等零散的关键词，而用户想要匹配整个短语的结果却在后面。 导致这件事的原因，是es搜索时使用 search_analyzer(分析器) 导致查询关键词被分词的策略导致的。执行查询时，es先把查询关键词经过 search_analyzer 设置的分析器分析，再把分析器得到的结果挨个放进 bool 查询中的 should 语句，这些 should 没有权重与顺序的差别，并且只要命中一个should 语句的文档都会被返回。所以导致 硬件设备需求表 这个词，在搜索时，实际上是用“硬件”、“设备”等这些词进行搜索。 解决方法match_phrase使用 match_phrase 搜索词组，match_phrase 查询会将查询字符串视为一个完整的短语，而不是将其拆分为词汇单元进行匹配。 以上是官方对于他的定义，但实际上 match_phrase 并不能去匹配完整的短语，因为es使用倒排索引进行搜索，而在倒排索引中存储的是分词之后的结果，根本没有原文。所以在使用match_phrase 时很可能会遇到搜索不到的情况。我将在下文详细讲解。 像这样我们就能搜索到想要的短语了。除此之外还可以使用 slop 属性来设置搜索短语的最短出现词间距。 优化查询权重目前产品中的es搜索查询权重均为1.0，查询字段没有权重之分。一般来说标题的权重应该高于正文的权重，准确匹配的权重高于模糊匹配。我们应该根据实际情况，将重要字段的权重提升到合理范围。 但是应该注意的是，我们的评分中含有用户点击量这个影响因子。应该控制这个分数的上限（或者是权重）。避免出现用户搜索 0.1 热度文档的匹配度为 1000 热度文档的 100 倍，但结果排名依然比不过 1000 热度的文档。 解决：使用 function_score 自定义分数时，用 field_value_factor 对于自定义的分数做后处理。可选参数为 none（默认值），不进行任何操作，直接使用 factor 指定的加权因子。 log，对加权因子进行对数运算，可以减小极大值和极小值之间的差距。 log1p，对加权因子加1后，进行对数运算。 log2p，对加权因子加2后，进行对数运算。 ln，使用自然对数函数 ln 进行运算。 square，对加权因子进行平方运算。 sqrt，对加权因子进行平方根运算。 结合此处不希望评分受到用户点击量过多导致评分过高的需求，应使用 log1p 对点击评分做标准化。下图为log1p的函数图像。 调整es的评分算法根据es文档，产品版本默认使用 BM25 算法作为es相似度算法。对于此算法本文不做解释。我们现在主要讨论怎样控制算法达到我们的要求。BM25有三个可选参数： k1：控制非线性术语频率归一化（饱和度）。默认值为1.2。b：控制文档长度对tf值进行归一化的程度。默认值为0.75。discount_overlaps：确定在计算标准化时是否忽略重叠的标记（位置增量为0的标记）。默认情况下为true，意味着在计算标准化时不考虑重叠的标记。 用人话来说就是，将 k1 的值从默认的1.2增加到1.5，增加非线性归一化的程度。这样做可以使得高频词的权重更突出。将 b 的值从默认的0.75减小到0.6，以减小文档长度对tf值的归一化程度。这样做可以使得较长的文档不会在评分上过于受到惩罚。discount_overlaps 默认为true，会忽略短时间重复出现的编辑，如果一个文档中有多个相同的词汇出现在相邻的位置，那么这些重叠的标记只会被计算一次。 具体到本次优化，可以适当降低 b 的值，使得 content 短的文档不被突出。 12345678910111213PUT /knowledge_document/_settings{ &quot;index&quot;: { &quot;similarity&quot;: { &quot;default&quot;: { &quot;type&quot;: &quot;BM25&quot;, &quot;k1&quot;: 1.2, &quot;b&quot;: 0.2, &quot;discount_overlaps&quot;: true } } }} 加入近义词、同义词ES提供了同义词过滤器，可以将同义词映射到一个共同的标记。通过在分析器（Analyzer）链中添加同义词过滤器，可以扩展搜索的匹配范围。例如，将”实验“和”试验“映射到同一个标记。扩大搜索范围。 但是中文字、词表达的意义多样，多音字、同义词又众多的情况下，靠开发团队很难实现快速的解决优化。 仍待解决的问题从原理上来说match_phrase仍然无法根治这个问题，match_phrase查找时，查找分词器分出的词的位置和要建索引时分出的词的位置一样。举个例子： 这里保存的数据是”Asdasd表1 测试标题搜索测试表格内容搜索测试正文搜索”而这个文件在es中的倒排索引长这样 点击查看 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154{ &quot;_index&quot; : &quot;knowledge_document&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;-2KG5IcBO2ayB5FAaPA0&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;took&quot; : 1, &quot;term_vectors&quot; : { &quot;fileContent&quot; : { &quot;field_statistics&quot; : { &quot;sum_doc_freq&quot; : 10161131, &quot;doc_count&quot; : 11714, &quot;sum_ttf&quot; : 51665947 }, &quot;terms&quot; : { &quot;1&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 2, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8 } ] }, &quot;asdasd&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 0, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 6 } ] }, &quot;内容&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 11, &quot;start_offset&quot; : 19, &quot;end_offset&quot; : 21 } ] }, &quot;搜索&quot; : { &quot;term_freq&quot; : 3, &quot;tokens&quot; : [ { &quot;position&quot; : 5, &quot;start_offset&quot; : 13, &quot;end_offset&quot; : 15 }, { &quot;position&quot; : 12, &quot;start_offset&quot; : 21, &quot;end_offset&quot; : 23 }, { &quot;position&quot; : 15, &quot;start_offset&quot; : 27, &quot;end_offset&quot; : 29 } ] }, &quot;标题&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 4, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13 } ] }, &quot;格&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 10, &quot;start_offset&quot; : 18, &quot;end_offset&quot; : 19 } ] }, &quot;正文&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 14, &quot;start_offset&quot; : 25, &quot;end_offset&quot; : 27 } ] }, &quot;测试&quot; : { &quot;term_freq&quot; : 3, &quot;tokens&quot; : [ { &quot;position&quot; : 3, &quot;start_offset&quot; : 9, &quot;end_offset&quot; : 11 }, { &quot;position&quot; : 6, &quot;start_offset&quot; : 15, &quot;end_offset&quot; : 17 }, { &quot;position&quot; : 13, &quot;start_offset&quot; : 23, &quot;end_offset&quot; : 25 } ] }, &quot;表&quot; : { &quot;term_freq&quot; : 2, &quot;tokens&quot; : [ { &quot;position&quot; : 1, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7 }, { &quot;position&quot; : 9, &quot;start_offset&quot; : 17, &quot;end_offset&quot; : 18 } ] }, &quot;表格&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 8, &quot;start_offset&quot; : 17, &quot;end_offset&quot; : 19 } ] }, &quot;试表&quot; : { &quot;term_freq&quot; : 1, &quot;tokens&quot; : [ { &quot;position&quot; : 7, &quot;start_offset&quot; : 16, &quot;end_offset&quot; : 18 } ] } } } }} 当我们使用 match_phrase 搜索 测试标题搜索 这个关键词时，实际上这个关键词还是会被分析器分词为”测试”,”标题”,”搜索”这样的关键词去搜索。match_phrase 查询强调术语在文档中的相对位置，只返回那些所有术语都按照指定顺序出现的文档。而match查询则更加宽松，只要文档包含了所有的术语，不论其顺序，都会被匹配和返回。 所以只有和下图一样同时含有”测试”,”标题”,”搜索”关键词，且前后位置关系相连的结果才会被返回。 但是当我们搜索另一个关键词 测试表格内容搜索 时，我们发现位置信息不再连续，导致使用match_phrase 也无法搜索到相关信息。如下图所示。 这是因为保存文档时使用了 ik_max_word 进行分词，分的比较细致多出了“试表”这样的分词，导致本来连续的关键词不再连续。解决方法就是使用 slop 参数来调节最小间隔。以避免这种情况下无法匹配到关键词。 使用match_phrase导致准确率反而下降例如，之前在在网上的梗 牙膏我只用中华为的就是刷牙干净速度快，每次只用5g就可以，本来是用来钓鱼的，可是同样也会坑到我们。我相信这样的正文放给无论什么样的分词器都无济于事，一定会分出“华为”、“5G”这样的关键词。而我们同时使用了match_phrase并给了其较高的权重。所以这样的噪声数据一定会被搜到。如下图。 进一步的优化方法更换更加先进的分词器本文使用 hanlp 作为分词器，进行了一系列测试，最终发现解决了一些问题，也引入了一些问题。 123456789101112131415161718import hanlp# 示例句子sentence = &quot;牙膏我只用中华为的就是刷牙干净速度快&quot;# 显示可用的分词模型，语种见名称最后一个字段或相应语料库print(hanlp.pretrained.tok.ALL)# 加载预训练模型进行分词任务tok = hanlp.load(hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH)# 分词结果sen_tok = tok(sentence)print(sen_tok)--------------------------------------------输出：['牙膏', '我', '只', '用', '中华', '为', '的', '就', '是', '刷牙', '干净', '速度', '快'] 以上是用过python调用hanlp自然语言分词，可以看到分词效果非常不错，同时还支持 COARSE_ELECTRA_SMALL_ZH 粗粒度 ，FINE_ELECTRA_SMALL_ZH 细粒度，两种模型。但是再将 hanlp 引入elasticSearch中，无法调用这些先进模型进行分词。 目前插件支持的分词类型有 12345678hanlp: hanlp默认分词hanlp_standard: 标准分词hanlp_index: 索引分词hanlp_nlp: NLP分词hanlp_crf: CRF分词hanlp_n_short: N-最短路分词hanlp_dijkstra: 最短路分词hanlp_speed: 极速词典分词 经过测试，hanlp_crf 分词器也能取得良好效果。但也存在一个问题，分词的粒度过大且无法调节，如下图，无法将 国家航空航天局 再分，会导致搜索时召回率下降。 目前正在探索如何将上文python中的分词方法带入es中。 NLP通过使用NLP技术，可以对用户输入的查询进行语义理解，而不仅仅是简单的关键词匹配。这可以帮助系统更好地理解用户的意图并提供更相关的搜索结果。例如，可以应用词义消歧算法来解决查询中的歧义，或者使用命名实体识别技术来识别重要的实体（如”风洞实验数据管理系统”中的”风洞实验”）。 此外我还注意到我们将文档中的关键词已经提取处理是否可以作为进一步优化搜索结果的判断条件呢？ 矢量搜索待最佳实践","link":"/post/improving-accuracy-of-elasticSearch/"},{"title":"解决Feign传递MultipartFile参数报错的问题","text":"本文介绍了在使用Feign进行RPC请求时，传递MultipartFile参数可能会导致”Current request is not a multipart request”错误的问题，并提供了相应的解决方案。 问题分析Feign通过HTTP协议进行远程调用，当传递MultipartFile参数时，需要将请求头设置为multipart/form-data类型，以便服务器正确解析参数。然而，默认情况下，Feign并不会自动将请求头设置为multipart/form-data，从而导致服务器无法正确处理MultipartFile参数，进而报错”Current request is not a multipart request”。 这个原因大概是Feign发起rpc请求时，没有将参数 MultipartFile 正确的设置为 multipart/form-data 类型，从而导致服务器不能正确的解析参数。 解决方案要解决这个问题，我们需要在Feign客户端和被调用服务端分别进行相应的配置。 Feign客户端方面代码在Feign客户端接口中，我们需要添加consumes = MediaType.MULTIPART_FORM_DATA_VALUE到@GetMapping注解中，同时，将MultipartFile参数的注解改为@RequestPart，以确保Feign正确处理MultipartFile参数。 1234567@FeignClient(&quot;example-service&quot;)public interface ExampleFeignClient { @GetMapping(value = &quot;/endpoint&quot;, consumes = MediaType.MULTIPART_FORM_DATA_VALUE) String getEndpointData(@RequestPart(&quot;file&quot;) MultipartFile file, @RequestParam(&quot;param2&quot;) int param2, @RequestParam(&quot;param3&quot;) boolean param3);} Feign被调用服务端在被调用服务端的控制器中，我们同样需要使用@RequestPart来接收MultipartFile参数，并确保Feign客户端传递参数时，请求头被设置为multipart/form-data类型。 ps:经过实测，在被调用方就算使用 @RequestParam 来接收MultipartFile参数，也是可以正常运行的。但是规范起见，建议还是使用 @RequestPart来接收MultipartFile参数 1234567891011121314151617@RestControllerpublic class ExampleController { private final ExampleFeignClient feignClient; @Autowired public ExampleController(ExampleFeignClient feignClient) { this.feignClient = feignClient; } @GetMapping(&quot;/call-remote-service&quot;) public String callRemoteService(@RequestPart(&quot;file&quot;) MultipartFile file, @RequestParam(&quot;param2&quot;) int param2, @RequestParam(&quot;param3&quot;) boolean param3) { return feignClient.getEndpointData(param1, param2, param3); }} 现在，Feign客户端和被调用服务端都正确配置了MultipartFile参数的传递方式。这个问题应该不会再出现。","link":"/post/Feign-passing-MultipartFile-error/"}],"tags":[{"name":"java","slug":"java","link":"/tags/java/"},{"name":"elasticSearch","slug":"elasticSearch","link":"/tags/elasticSearch/"}],"categories":[{"name":"Blog","slug":"Blog","link":"/categories/Blog/"},{"name":"踩坑记录","slug":"踩坑记录","link":"/categories/%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"}],"pages":[]}